# -*- coding: utf-8 -*-
"""RMA_Project_Clean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eYbJDnGhTieBucF7UcnUdJrqV1xwMalR
"""

# import some common libraries
import copy
import cv2
import numpy as np
import torch
import time

# import some specialty libraries
import face_recognition
import glob

# import some common detectron2 utilities
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

print("\ntorch.version:", torch.version, "\n")
print("torch.version.cuda:", torch.version.cuda, "\n")
print("torch.cuda.is_available():", torch.cuda.is_available(), "\n")
print("torch._C._cuda_getDeviceCount():", torch._C._cuda_getDeviceCount(), "\n")

# a collection of functions to create common model architectures listed in
# MODEL_ZOO.md, and optionally load their pre-trained weights
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

class Image_Analyzer:
  """
  A class for simplified use of detectron2's predictor, tailored to the specific
  needs of the SOCMINTEX project.
  """

  def __init__(self):
    """
    Initializes the model used for panoptic segmentation, this step is only
    required once per session of analysis. Hence, it takes place in the
    constructor.
    """
    # The parameters are locked behind private access, to limit unintended use
    self.__cfg = get_cfg()
    self.__cfg.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
    self.__cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
    self.__predictor = DefaultPredictor(self.__cfg)

  def get_predictor(self):
    """
    Getter function for the predictor.
    """
    return self.__predictor

  def analyze(self, img, return_display:bool=False, other_functions:list=[]):
    """
    Takes an image, performs panoptic segmentation on it,
    and returns a list of identified objects within it. For each object
    in the list, if it is a thing (as defined by the COCO dataset),
    the description contains 3 fields:
    'category': the name of the thing
    'confidence': its confidence score
    'area': the portion of the image that the object takes up, from 0 to 1
    If instead, the object is stuff (as defined by the COCO dataset), meaning
    an uncountable substance, then it has the same parameters as a thing,
    minus the confidence score.

    Parameters:
    -----------
    img : numpy.ndarray
      The image to be analyzed.
    return_display : bool, optional
      Used to request an annotation of the image.
    """
    # Call on detectron2's basic predictor to perform the analysis
    panoptic_seg, segments_info = self.__predictor(img)["panoptic_seg"]

    # Determine the proportions of each class present in the image matrix
    panoptic_seg_np = panoptic_seg.detach().cpu().numpy()
    breakdown = {}
    for i in range(1, len(segments_info)+1):
      breakdown[i] = np.count_nonzero(panoptic_seg_np==i)
    img_area = img.shape[0]*img.shape[1]

    # Create the new list of object descriptions using basic detectron2 results
    results = []
    for i in range(len(segments_info)):
      # Only 'thing' classes have 'instance_id'
      if 'instance_id' in segments_info[i].keys():
        thing = {'category':0, 'area':0, 'confidence':0}
        thing['category'] = MetadataCatalog.get(self.__cfg.DATASETS.TRAIN[0]).thing_classes[segments_info[i]['category_id']]
        thing['confidence'] = segments_info[i]['score']
        thing['area'] = breakdown[segments_info[i]['id']]/img_area
        results.append(thing)
      else:
        stuff = {'category':0, 'area':0}
        stuff['category'] = MetadataCatalog.get(self.__cfg.DATASETS.TRAIN[0]).stuff_classes[segments_info[i]['category_id']]
        stuff['area'] = breakdown[segments_info[i]['id']]/img_area
        results.append(stuff)
    unidentified_area = 1 - (sum(breakdown.values())/img_area)
    results.append({'category':'unidentified', 'area': unidentified_area})

    # An optional feature to perform further analysis on the image
    if other_functions!=[]:
      for function in other_functions:
        # Results must be presented in a way to be added to the results list
        result = function(img)
        results.append(result)

    if not return_display:
      return results

    # Draw the panoptic segmentation and return the result
    else:
      v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(self.__cfg.DATASETS.TRAIN[0]), scale=1.2)
      out = v.draw_panoptic_seg_predictions(panoptic_seg.to("cpu"), segments_info)
      annotated_img = out.get_image()[:, :, ::-1]
      resized_annotation = cv2.resize(annotated_img, img.shape[1::-1])
      comparison = cv2.hconcat([img, resized_annotation])
      # The percentage of unidentified area is written in purple on the image
      cv2.putText(comparison, str(round(100*unidentified_area, 2))+'%', (int(comparison.shape[1]/2), 40), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 255), 3)
      return results, comparison

  def analyze_multiple(self, directory0:str, n:int, file_path, directory1:str=""):
    """
    Runs the analyze() method on n images present in the
    directory. The results are then converted to strings and printed into a
    .txt document at the given file_path. If a directory1 is provided, it means
    that the annotated images are expected to be created and saved at this
    second location.

    Parameters:
    -----------
    directory0 : str
      The path of the directory containing the collection of images
      to be analyzed.
    n : int
      The number of images to be analyzed from the directory. If it
      surpasses the size of the directory, then the entire directory is
      analyzed.
    file_path : str
      The destination path for the .txt file containing the results of the
      analysis.
    directory1 : str, optional
      The destination directory for the annotated images, comprised of a side-by-side
      view of the original image and the segmented one.
    """
    t0 = time.time()
    glob_path = directory0+'/*'
    paths = glob.glob(glob_path)
    if len(paths)<n:
      n = len(paths)
    # If no second file path is provided, proceed as normal
    if directory1=="":
      f = open(file_path, "w")
      i=0
      while i<n:
        res = self.analyze(cv2.imread(paths[i]))
        f.write(str(res)+"\n")
        print(str(round(100*(i/n), 2))+"%")
        i+=1
      print("Running this function took:", time.time()-t0, "seconds.")
      f.close()

    # If a second file path is provided, the annotated images must be stored
    # WITH THIS SETUP, I SHOULD BE ABLE TO ORDER IMAGES ALPHABET. = SAME ORDER AS
    # ANNOTATIONS ON THE RESULTS DOCUMENT
    else:
      f = open(file_path, "w")
      i=0
      while i<n:
        image_file = paths[i].split(directory0+"/")[-1]
        name = "Comparison_"+str(i)+"_"+image_file.split(".")[0]+".jpg"
        new_img_path = directory1+"/"+name
        res, comparison = self.analyze(cv2.imread(paths[i]), True, [])
        f.write(str(res)+"\n")
        cv2.imwrite(new_img_path, comparison)
        print("We have just printed an image at", new_img_path)
        print(str(round(100*(i/n), 2))+"%")
        i+=1
      print("Running this function took:", time.time()-t0, "seconds.")
      f.close()

class Results_Presenter:
  """
  A class for consolidating current and future methods of presenting,
  organizing, or analyzing the results of the analyze() method in the
  Image_Analyzer class.
  """

  def results2table(results:list):
    """
    Presents the list of objects detected in an image in a pandas table.

    Parameters:
    -----------
    results : list
      The list returned by the analyze() method in Image_Analyzer().
    """
    results_copy = copy.deepcopy(results)
    index_labels = []
    for res in results_copy:
      index_labels.append(res['category'])
      res.pop('category')
    df = pd.DataFrame(results_copy)
    df.index = index_labels
    return df

  def word_vectors(results:list, min_conf:float, min_area:float):
    """
    Filters the results of a COCO image analysis by confidence score for
    'things' and area for 'stuff'. It then generates two word vectors,
    their keys are the names of the classes found in the image.
    The things_vector has the number of instances of each thing class for its
    values.
    The stuff_vector

    Parameters:
    -----------
    results : list
      The list returned by the analyze() method in Image_Analyzer().
    min_conf : float
      The threshold of confidence score needed to be a valid entry.
    min_area : float
      The proportion of an image that needs to be occupied to be valid.
    """
    things_vector = {}
    stuff_vector = {}
    for obj in results:
      # Keep likely things
      if ('confidence' in obj.keys()) and (obj['confidence']>=min_conf):
        if obj['category'] in things_vector.keys():
          things_vector[obj['category']] += 1
        else:
          things_vector[obj['category']] = 1
      # Keep large-enough stuff
      elif ('confidence' not in obj.keys()) and (obj['area']>=min_area):
        stuff_vector[obj['category']] = int(round(10*obj['area']/min_area, 0))
    return things_vector, stuff_vector

viewer = Image_Analyzer()
viewer.analyze_multiple("/mnt/userdata/nlr/Project/MyPersonalTestImages/Images", 6000, "/mnt/userdata/nlr/Project/Results/Analysis_simple.txt", "/mnt/userdata/nlr/Project/MyPersonalTestImages/Annotated_Images")